# Erste Bank Scorecard â€” Demo Data Pipeline (OOP, SQLite, Pandas)
A data engineering project that simulates daily banking data, implements basic quality, stages raw files, builds a typed clean layer, and produces a per-application performance table enriched with reference data along with some metrics. These are stored in a daily csv report file.

---

### **Project Structure**

```
erste_bank_credit_scorecard_pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ common/                     # Shared utilities (audit)
â”‚   â”‚   â”œâ”€â”€ audit.py                    # AuditRepository class
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_generate/              # Synthetic data generation
â”‚   â”‚   â”œâ”€â”€ data_generator.py           # EsteDataGenerator class
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_quality/               # Validation framework
â”‚   â”‚   â”œâ”€â”€ data_quality_checks.py      # DataQualityChecker class
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_load/                  # Staging layer
â”‚   â”‚   â”œâ”€â”€ data_staging.py             # SQLiteStagingLoader class
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_clean/                 # Cleaning layer
â”‚   â”‚   â”œâ”€â”€ data_cleaning.py            # SQLiteCleaner class
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ fact_dim/                   # Fact + Dimensions layer (business tables)
â”‚   â”‚   â”œâ”€â”€ dim_reference_tables.py     # ReferenceTablesManager class
â”‚   â”‚   â”œâ”€â”€ fact_application_performance.py  # ApplicationPerformanceBuilder class
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_mart/                  # Metrics calculation
â”‚   â”‚   â”œâ”€â”€ scorecard_metrics.py        # ScorecardMetricsCalculator class
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ pipeline_executor.py        # Main orchestration script
â”œâ”€â”€ config/                         # Configuration files
â”‚   â”œâ”€â”€ data_quality_schema.json       # DQ validation rules
â”‚   â””â”€â”€ pipeline_config.yaml           # Business dimensions config
â”œâ”€â”€ output/                         # Business reports
â”‚   â””â”€â”€ scorecard_metrics_YYYY-MM-DD.csv
â”œâ”€â”€ quality_output/                 # Data quality reports
â”‚   â””â”€â”€ dq_report_YYYY-MM-DD.json
â”œâ”€â”€ logs/                           # Execution logs
â”‚   â””â”€â”€ dq_YYYY-MM-DD.log
â”œâ”€â”€ db/                             # SQLite database
â”‚   â””â”€â”€ erste_scorecard.db
â”œâ”€â”€ erste_bank_data/               # Daily raw data (partitioned by run date)
â”‚   â””â”€â”€ YYYY-MM-DD/
â”‚       â”œâ”€â”€ applications.csv
â”‚       â”œâ”€â”€ accounts.csv
â”‚       â”œâ”€â”€ transactions.csv
â”‚       â”œâ”€â”€ payments.csv
â”‚       â””â”€â”€ delinquency.csv
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                     # This file
```
---

## Features
- **Daily batch** pipeline, partitioned by `run_date` (`YYYY-MM-DD`)
- **Config-driven DQ gate** (required columns, enums, ranges, non-negative, date formats, duplicate IDs)
- **Schema-drift tolerant staging** on SQLite with dynamic column adds
- **Typed, idempotent clean layer** with window-based dedup (latest record wins)
- **Performance mart**: `application_performance` joins clean tables + reference lookups
- **Audit**: run status + per-table load counts; metadata columns `_run_date`, `_source_file`, `_ingested_at`, `_batch_id`

---

## Prerequisites
- Python 3.10+
- SQLite (bundled with Python via `sqlite3`)

### Install dependencies
pip install -r requirements.txt

### Clone the repository
git clone https://github.com/alrammos91/erste_bank_credit_scorecard_pipeline.git
cd erste_bank_credit_scorecard_pipeline

---

### Run the Pipeline
```bash
# Generate 200 applications (default)
python src/pipeline_executor.py

# Custom run with 500 applications
python src/pipeline_executor.py --n-apps 500 --seed 123

```


## **Output Files**

### **Business Metrics**
- **File**: `output/scorecard_metrics_YYYY-MM-DD.csv`
- **Purpose**: Compare pilot vs. existing scorecard performance
- **Usage**: Import to Excel, filter by dimensions, create pivot tables

### **ğŸ” Data Quality Reports**
- **File**: `quality_output/dq_report_YYYY-MM-DD.json`
- **Purpose**: Comprehensive data validation results
- **Content**: Schema compliance, data integrity checks, error details

### **ğŸ“ Execution Logs**
- **File**: `logs/dq_YYYY-MM-DD.log`
- **Purpose**: Detailed pipeline execution tracking
- **Content**: Data quality checks, processing steps, audit information

---

## **Configuration Management**

### **Business Dimensions** (`config/pipeline_config.yaml`)
```yaml
dimensions:
  products:
    - "standard"
    - "gold" 
    - "platinum"
    
  channels:
    - "online"
    - "branch"
    
  segments:
    - "retail"
    - "student"
```


## **Pipeline Workflow**

1. **Data Generation**: Create realistic synthetic credit applications
2. **Quality Validation**: Schema compliance and business rule checks
3. **Stage Layer**: Raw data staging with audit trails
4. **Clean Layer**: Cleaned and standardized data
5. **Fact-Dim Layer**: Business-optimized fact and dimension tables
6. **Metrics Calculation**: 6 scorecard KPIs with dimensional breakdown
7. **Report Generation**: Business-ready CSV for stakeholder analysis

---




